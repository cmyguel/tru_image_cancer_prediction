{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_dataset(source_dir: str, split_ratio: float = 0.8, image_ext: str = \"*.jpg\"):\n",
    "    \"\"\"\n",
    "    Splits a dataset into training and validation sets.\n",
    "\n",
    "    Parameters:\n",
    "        source_dir (str): Path to the original dataset containing 'original' subdirectory.\n",
    "        split_ratio (float): Percentage of images used for training (default: 0.8).\n",
    "        image_ext (str): Image file extension to search for (default: \"*.jpg\").\n",
    "    \"\"\"\n",
    "    source_path = Path(source_dir) / \"original\"\n",
    "    train_path = Path(source_dir) / \"train\"\n",
    "    val_path = Path(source_dir) / \"val\"\n",
    "\n",
    "    categories = [category.name for category in source_path.iterdir() if category.is_dir()]\n",
    "\n",
    "    # Create train and val directories\n",
    "    for category in categories:\n",
    "        (train_path / category).mkdir(parents=True, exist_ok=True)\n",
    "        (val_path / category).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Process each category\n",
    "    for category in categories:\n",
    "        image_files = list((source_path / category).glob(image_ext))\n",
    "        random.shuffle(image_files)\n",
    "\n",
    "        split_index = int(len(image_files) * split_ratio)\n",
    "        train_files = image_files[:split_index]\n",
    "        val_files = image_files[split_index:]\n",
    "\n",
    "        # Copy images to respective folders\n",
    "        for file in train_files:\n",
    "            shutil.copy(file, train_path / category / file.name)\n",
    "\n",
    "        for file in val_files:\n",
    "            shutil.copy(file, val_path / category / file.name)\n",
    "\n",
    "        print(f\"Category '{category}': {len(train_files)} train, {len(val_files)} val\")\n",
    "\n",
    "    print(\"Dataset successfully split into training and validation sets.\")\n",
    "\n",
    "# Split if output folders don't exist already\n",
    "if not Path(\"brain_tumor_dataset/train\").exists():\n",
    "    split_dataset(\"brain_tumor_dataset\", split_ratio=0.8, image_ext=\"*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no', 'yes']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = 'brain_tumor_dataset'\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AsusTUF\\anaconda3\\envs\\tru_image_cancer_prediction\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\AsusTUF\\anaconda3\\envs\\tru_image_cancer_prediction\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze all the parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the classifier\n",
    "num_features = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_features, 2)  # Assuming binary classification\n",
    "\n",
    "model = model.to(device)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Only parameters of the final layer are being optimized\n",
    "# optimizer = optim.SGD(model.classifier[6].parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(model.classifier[6].parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_model(model, criterion, optimizer, dataloaders, dataset_sizes, device, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_f1 = 0.0  # Track best F1-score\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluation mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            all_labels = []\n",
    "            all_preds = []\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Store labels and predictions for F1-score\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            # Compute F1-score\n",
    "            epoch_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1: {epoch_f1:.4f}')\n",
    "\n",
    "            # Save model if it has the best F1-score\n",
    "            if phase == 'val' and epoch_f1 > best_f1:\n",
    "                best_f1 = epoch_f1\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val F1: {best_f1:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.4867 Acc: 0.7897 F1: 0.7544\n",
      "val Loss: 0.3261 Acc: 0.9000 F1: 0.8949\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.4358 Acc: 0.7897 F1: 0.7832\n",
      "val Loss: 0.3280 Acc: 0.8600 F1: 0.8498\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.5031 Acc: 0.7436 F1: 0.6962\n",
      "val Loss: 0.3377 Acc: 0.7800 F1: 0.7512\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.5048 Acc: 0.7795 F1: 0.7640\n",
      "val Loss: 0.3310 Acc: 0.9200 F1: 0.9167\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.4800 Acc: 0.8000 F1: 0.7820\n",
      "val Loss: 0.3093 Acc: 0.9000 F1: 0.8949\n",
      "\n",
      "Training complete in 3m 30s\n",
      "Best val F1: 0.9167\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer, dataloaders, dataset_sizes, device, num_epochs=5) # num_epochs=25 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7426, -2.2357],\n",
       "        [ 0.0782, -1.0827],\n",
       "        [ 0.9488, -1.2596],\n",
       "        [-1.4362,  1.4291],\n",
       "        [-0.2174,  0.3437],\n",
       "        [ 0.5777, -1.1007],\n",
       "        [ 1.7262, -2.9884],\n",
       "        [-2.3679,  2.0415],\n",
       "        [ 0.0793, -0.7718],\n",
       "        [-2.4263,  1.6497],\n",
       "        [-0.7366,  0.3211],\n",
       "        [-0.8009,  0.5786],\n",
       "        [-1.3129,  1.9209],\n",
       "        [-0.2972,  0.5597],\n",
       "        [ 1.1532, -1.2753],\n",
       "        [-0.5341, -0.3901],\n",
       "        [ 1.4990, -1.6322],\n",
       "        [ 2.0352, -2.6346]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for inputs, labels in dataloaders['val']:\n",
    "    y_pred = model(inputs)\n",
    "y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: yes\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "model.eval()\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Ensure it's RGB\n",
    "    image = transform(image)  # Apply transformations\n",
    "    image = image.unsqueeze(0)  # Add batch dimension (1, C, H, W)\n",
    "    return image\n",
    "\n",
    "\n",
    "def predict(image_path, model, class_names, device):\n",
    "    image = preprocess_image(image_path).to(device)  # Send image to device (GPU/CPU)\n",
    "    \n",
    "    # Perform inference\n",
    "    model.eval()  # Ensure model is in eval mode\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted_class = torch.max(output, 1)  # Get the class index\n",
    "\n",
    "    return class_names[predicted_class.item()]  # Return class label\n",
    "\n",
    "# Define class names based on training data\n",
    "class_names = [\"no\", \"yes\"]  # Modify based on your dataset structure\n",
    "\n",
    "# Path to a sample MRI image\n",
    "sample_image = \"brain_tumor_dataset/val/yes/Y6.jpg\"  # Replace with an actual image path\n",
    "\n",
    "# Run inference\n",
    "predicted_label = predict(sample_image, model, class_names, device)\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50])\n",
      "validation labels:\n",
      "tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
      "        1, 0])\n",
      "validation probs:\n",
      "tensor([0.9880, 0.8528, 0.5359, 0.9536, 0.2383, 0.0444, 0.3597, 0.9580, 0.9609,\n",
      "        0.2840, 0.9420, 0.9739, 0.2992, 0.1738, 0.6367, 0.0181, 0.2385, 0.8792,\n",
      "        0.8150, 0.0089, 0.5652, 0.8885, 0.7020, 0.0587, 0.7989, 0.9463, 0.8787,\n",
      "        0.1573, 0.9461, 0.5362, 0.8940, 0.9783, 0.0418, 0.4019, 0.0736, 0.9797,\n",
      "        0.0990, 0.1118, 0.0093, 0.6828, 0.3422, 0.9833, 0.0184, 0.7118, 0.9621,\n",
      "        0.0810, 0.6749, 0.7422, 0.9514, 0.1080])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_proba(model, dataloader, device):\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)  # Move input to GPU/CPU\n",
    "            outputs = model(inputs)  # Get raw model outputs (logits)\n",
    "\n",
    "            # Apply Softmax if using CrossEntropyLoss (multi-class)\n",
    "            probs = F.softmax(outputs, dim=1)[:, 1]  # Extract probability of positive class (index 1)\n",
    "\n",
    "            all_probs.append(probs.cpu())  # Move to CPU and store\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "\n",
    "    return torch.cat(all_labels), torch.cat(all_probs)  # Concatenate all probabilities into a single tensor\n",
    "\n",
    "# Run inference on validation dataset\n",
    "val_labels, val_probs = predict_proba(model, dataloaders['val'], device)\n",
    "\n",
    "# Print shape of output tensor\n",
    "print(val_probs.shape)  # Should match number of validation samples\n",
    "print(\"validation labels:\")\n",
    "print(val_labels)\n",
    "print(\"validation probs:\")\n",
    "print(val_probs)  # Display tensor of probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tru_image_cancer_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
